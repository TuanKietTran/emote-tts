@article{an2021emotional,
  title     = {Emotional Text-to-Speech Synthesis: A Review of Methods and Techniques},
  author    = {An, G. and Wu, L. and Xie, J. and Huang, M. and Xie, X.},
  journal   = {IEEE Access},
  year      = {2021},
  volume    = {9},
  pages     = {10001--10019},
  doi       = {10.1109/ACCESS.2021.3049256},
}

@article{nose2007style,
  author    = {Nose, T. and Yamagishi, J. and Masuko, T. and Kobayashi, T.},
  title     = {A style control technique for hmm-based expressive speech synthesis},
  journal   = {IEICE Transactions on Information and Systems},
  volume    = {90},
  pages     = {1406--1413},
  year      = {2007}
}

@inproceedings{henter2017principles,
  author    = {Henter, G. E. and Lorenzo-Trueba, J. and Wang, X. and Yamagishi, J.},
  title     = {Principles for learning controllable TTS from annotated and latent variation},
  booktitle = {Proc. Interspeech},
  pages     = {3956--3960},
  year      = {2017}
}

@article{lorenzotrueba2018investigating,
  author    = {Lorenzo-Trueba, J. and Henter, G. E. and Takaki, S. and Yamagishi, J. and Morino, Y. and Ochiai, Y.},
  title     = {Investigating different representations for modeling and controlling multiple emotions in DNN-based speech synthesis},
  journal   = {Speech Communication},
  year      = {2018}
}

@article{wang2018style,
  author    = {Wang, Y. and Stanton, D. and Zhang, Y. and Skerry-Ryan, R. and Battenberg, E. and Shor, J. and Xiao, Y. and Ren, F. and Jia, Y. and Saurous, R.},
  title     = {Style tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis},
  journal   = {arXiv preprint arXiv:1803.09017},
  year      = {2018}
}

@article{skerry2018towards,
  author    = {Skerry-Ryan, R. and Battenberg, E. and Xiao, Y. and Wang, Y. and Stanton, D. and Shor, J. and Weiss, R. and Clark, R. and Saurous, R.},
  title     = {Towards end-to-end prosody transfer for expressive speech synthesis with Tacotron},
  journal   = {arXiv preprint arXiv:1803.09047},
  year      = {2018}
}

@misc{wu2018feature,
  author    = {Wu, X. and Sun, L. and Kang, S. and Liu, S. and Wu, Z. and Liu, X. and Meng, H.},
  title     = {Feature based adaptation for speaking style synthesis},
  howpublished = {Manuscript},
  year      = {2018}
}

@inproceedings{wu2018rapid,
  author    = {Wu, X. and Cao, Y. and Wang, M. and Liu, S. and Kang, S. and Wu, Z. and Liu, X. and Su, D. and Yu, D. and Meng, H.},
  title     = {Rapid style adaptation using residual error embedding for expressive speech synthesis},
  booktitle = {Proc. Interspeech},
  pages     = {3072--3076},
  year      = {2018}
}

@article{liu2020multi,
  author    = {Liu, Songxiang and Cao, Yuewen and Meng, Helen},
  title     = {Multi-target emotional voice conversion with neural vocoders},
  journal   = {arXiv preprint arXiv:2004.03782},
  year      = {2020}
}


@inproceedings{oord2016wavenet,
  title={WaveNet: A Generative Model for Raw Audio},
  author={van den Oord, Aaron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  booktitle={Proceedings of the 9th ISCA Speech Synthesis Workshop},
  pages={125},
  year={2016},
  organization={ISCA},
  doi={10.48550/arXiv.1609.03499},
  url={https://arxiv.org/abs/1609.03499}
}

@inproceedings{jang2021univnet,
  title={UnivNet: A Neural Vocoder with Multi-Resolution Spectrogram Discriminators for High-Fidelity Waveform Generation},
  author={Jang, Won and Lim, Dan and Yoon, Jaesam and Kim, Bongwan and Kim, Juntae},
  booktitle={Proceedings of Interspeech 2021},
  pages={2207--2211},
  year={2021},
  organization={ISCA},
  doi={10.21437/Interspeech.2021-1171},
  url={https://arxiv.org/abs/2106.07889}
}


@article{siuzdak2022vocos,
  title={Vocos: Closing the Gap Between Time-Domain and Fourier-Based Neural Vocoders for High-Quality Audio Synthesis},
  author={Siuzdak, Hubert},
  journal={arXiv preprint arXiv:2201.08337},
  year={2022},
  doi={10.48550/arXiv.2201.08337},
  url={https://arxiv.org/abs/2201.08337}
}


@article{albadawy2021vocbench,
  title={VocBench: A Neural Vocoder Benchmark for Speech Synthesis},
  author={AlBadawy, Ehab A. and Gibiansky, Andrew and He, Qing and Wu, JiLong and Chang, Ming-Ching and Lyu, Siwei},
  journal={arXiv preprint arXiv:2112.03099},
  year={2021},
  doi={10.48550/arXiv.2112.03099},
  url={https://arxiv.org/abs/2112.03099}
}

@article{zarazaga2022neural,
  title={Speaker-Independent Neural Formant Synthesis},
  author={PÃ©rez Zarazaga, Pablo and Malisz, Zofia and Henter, Gustav Eje and Juvela, Lauri},
  journal={arXiv preprint arXiv:2203.17032},
  year={2022},
  doi={10.48550/arXiv.2203.17032},
  url={https://arxiv.org/abs/2203.17032}
}


@article{lei2022msemotts,
  title={Msemotts: Multi-scale emotion transfer, prediction, and control for emotional speech synthesis},
  author={Lei, Yi and Yang, Shan and Wang, Xinsheng and Xie, Lei},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={30},
  pages={853--864},
  year={2022},
  publisher={IEEE}
}

@inproceedings{raptis2014expressive,
  author    = {Raptis, Spyros and Karabetsos, Sotiris and Chalamandaris, Aimilios and Tsiakoulis, Pirros},
  title     = {Towards Expressive Speech Synthesis: Analysis and Modeling of Expressive Speech},
  booktitle = {Proceedings of the 2014 5th IEEE Conference on Cognitive Infocommunications (CogInfoCom)},
  year      = {2014},
  pages     = {461--465},
  keywords  = {Speech, Acoustics, Hidden Markov models, Feature extraction, Speech synthesis, Analytical models, emotion classification, emotional speech, expressive speech, Text-to-Speech, acoustic analysis, speech synthesis},
  doi       = {10.1109/CogInfoCom.2014.7020500}
}

@article{wang2017tacotron,
  title={Tacotron: Towards End-to-End Speech Synthesis},
  author={Wang, Yuxuan and Skerry-Ryan, RJ and Stanton, Daisy and Wu, Yonghui and Weiss, Ron J and Jaitly, Navdeep and Yang, Zongheng and Xiao, Ying and Chen, Zhenyao and Bengio, Samy and others},
  journal={arXiv preprint arXiv:1703.10135},
  year={2017}
}

@inproceedings{kong2020hifi,
  title={HiFi-GAN: Generative adversarial networks for efficient and high fidelity speech synthesis},
  author={Kong, Jungil and Kim, Jaehyeon and Bae, Jaekyoung},
  booktitle={Advances in Neural Information Processing Systems},
  pages={17087--17096},
  year={2020}
}

@article{zhang2021deep,
  title={Deep learning based emotional speech synthesis: A review},
  author={Zhang, Xu and Xu, Ming and Cui, Jianjun and others},
  journal={arXiv preprint arXiv:2106.08395},
  year={2021}
}


@inproceedings{zhou2020transforming,
  title={Transforming spectrum and prosody for emotional voice conversion with non-parallel training data},
  author={Zhou, Kun and Yamagishi, Junichi and Veaux, Christophe},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={7754--7758},
  year={2020},
  organization={IEEE}
}

@inproceedings{kim2021conditional,
  title={Conditional variational autoencoder with attention for emotional speech synthesis},
  author={Kim, Daehan and Ahn, Dongjun and Kim, Heejin and Kim, Nam Soo},
  booktitle={2021 IEEE Spoken Language Technology Workshop (SLT)},
  pages={533--540},
  year={2021},
  organization={IEEE}
}